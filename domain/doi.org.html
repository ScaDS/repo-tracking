
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Doi.org (52) &#8212; ScaDS.AI Zenodo Materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/searchbox.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/0.6.0/lunr.min.js"></script>
    <script src="../_static/js/searchbox.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'domain/doi.org';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Github.com (142)" href="github.com.html" />
    <link rel="prev" title="Mit license (27)" href="../licenses/mit_license.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="2025-12-17"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../readme.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ScaDS.AI Zenodo Materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ScaDS.AI Zenodo Materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../readme.html">
                    ScaDS.AI Code, Data and Model Repositories
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">What's new</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../whats_new.html">Recently added (10)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">How to contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/format.html">YML format</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By license</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../licenses/apache_license_2.0.html">Apache license 2.0 (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/bsd_3-clause_%22new%22_or_%22revised%22_license.html">Bsd 3-clause “new” or “revised” license (33)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/bsd-3-clause.html">Bsd-3-clause (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-4.0.html">Cc-by-4.0 (49)</a></li>





<li class="toctree-l1"><a class="reference internal" href="../licenses/creative_commons_attribution_4.0_international.html">Creative commons attribution 4.0 international (18)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/mit_license.html">Mit license (27)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By domain</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Doi.org (52)</a></li>





<li class="toctree-l1"><a class="reference internal" href="github.com.html">Github.com (142)</a></li>
<li class="toctree-l1"><a class="reference internal" href="zenodo.org.html">Zenodo.org (52)</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../statistics/readme.html">Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../export/readme.html">Open data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gdpr_compliance.html">GDPR Compliance Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../imprint.html">Imprint</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/scads/zenodo-tracking" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/scads/zenodo-tracking/issues/new?title=Issue%20on%20page%20%2Fdomain/doi.org.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/domain/doi.org.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Doi.org (52)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Doi.org (52)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-4-se-exploring-hyperparameter-usage-and-tuning-in-machine-learning-research-cain-2023-submission">AI-4-SE/Exploring-Hyperparameter-Usage-And-Tuning-In-Machine-Learning-Research: CAIN-2023 Submission</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai4psychology-ki-kompetenz-training">AI4Psychology KI-Kompetenz-Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autonomous-ai-systems-for-data-analysis">Autonomous AI Systems for Data Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-science-lectures-2025-uni-leipzig-scads-ai">Bio-image Data Science Lectures 2025 @ Uni Leipzig / ScaDS.AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-literacy-open-science-and-collaborative-coding-and-ci-cd-slides-for-the-scads-ai-ga-workshop-working-with-data-and-code-like-a-pro">Data literacy, Open Science and Collaborative Coding and CI/CD - Slides for the ScaDS.AI GA-Workshop “Working with data and code like a pro”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#devops-hands-on-training">DevOps Hands-on Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#emnlp-23-bootstrapping-a-violence-detector-for-fan-fiction">EMNLP-23-Bootstrapping-a-Violence-Detector-for-Fan-Fiction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#european-union-artificial-intelligence-act-in-a-nutshell">European Union Artificial Intelligence Act (in a nutshell)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-machine-learning">Explainable Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-generative-ai-impacts-research-and-teaching">How Generative AI impacts research and teaching</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-llms-impact-bioimage-data-science">How LLMs impact BioImage Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-large-language-models-impact-bio-image-data-science">How Large Language Models impact Bio-Image Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ir-lab-leipzig-jena-summer-term-2023">IR Lab Leipzig Jena Summer Term 2023</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#this-loads-a-patched-version-of-ir-datasets-that-can-load-resources-from-tira">this loads a patched version of ir_datasets that can load resources from TIRA</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#this-patches-ir-datasets-and-loads-pyterrier-so-that-it-can-load-resources-from-tira-and-can-run-in-the-tira-sandbox">this patches ir_datasets and loads PyTerrier so that it can load resources from TIRA and can run in the TIRA sandbox</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ir-lab-leipzig-jena-winter-term-2023-2024">IR Lab Leipzig/Jena Winter Term 2023/2024</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">this loads a patched version of ir_datasets that can load resources from TIRA</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">this patches ir_datasets and loads PyTerrier so that it can load resources from TIRA and can run in the TIRA sandbox</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ki-kompetenz-training">KI-Kompetenz-Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-embeddings-of-stable-diffusion-prompts">Manipulating Embeddings of Stable Diffusion Prompts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pan23-trigger-detection">PAN23 Trigger Detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering-techniques-slides-for-the-scads-ai-ga-workshop-coding-effectively-with-ai-getting-started-with-cursor-and-copilot">Prompt Engineering Techniques - Slides for the ScaDS.AI GA-Workshop “Coding effectively with AI: Getting started with Cursor and Copilot”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducible-bio-image-analysis-using-python-napari-jupyter-and-ai">Reproducible Bio-Image Analysis using Python, Napari, Jupyter and AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-management">Research Data Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-sentiment-classification-train-dev-test-pair-ids">Same Sentiment Classification Train/Dev/Test Pair IDs</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#example-access-to-single-business-review-id">example access to single business/review id</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-side-stance-classification-adversarial-test-cases">Same Side Stance Classification Adversarial Test Cases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-side-stance-classification-resampled-datasets">Same Side Stance Classification Resampled Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-4-2025-ai-insights-on-ai-agents-agent-frameworks">ScaDS.AI Meetup #4 2025: AI Insights on AI Agents &amp; Agent-Frameworks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-4-2025-workshop-ai-agents">ScaDS.AI Meetup #4 2025: Workshop AI Agents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-8-2025-ai-insights-model-context-protocol-mcp">ScaDS.AI Meetup #8 2025: AI Insights Model Context Protocol (MCP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-8-2025-workshop-model-context-protocol-mcp">ScaDS.AI Meetup #8 2025: Workshop Model Context Protocol (MCP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selected-meta-data-of-accepted-contributions-to-some-ai-ml-conferences-2020-2025">Selected meta data of accepted contributions to some AI/ML conferences 2020-2025</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-text-active-learning-for-text-classification-in-python">Small-Text: Active Learning for Text Classification in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#smart-slide-generation-re-using-training-materials-through-the-power-of-llms">Smart Slide Generation: Re-Using Training Materials Through the Power of LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#socio-technical-anti-patterns-in-building-ml-enabled-software-supplementary-material">Socio-Technical Anti-Patterns in Building ML-Enabled Software (Supplementary Material)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche-25-advertisement-in-retrieval-augmented-generation">Touché-25-Advertisement-in-Retrieval-Augmented-Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche20-argument-retrieval-for-controversial-questions">Touché20-Argument-Retrieval-for-Controversial-Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche21-argument-retrieval-for-controversial-questions">Touché21-Argument-Retrieval-for-Controversial-Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche22-image-retrieval-for-arguments">Touché22-Image-Retrieval-for-Arguments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#treesway">TreeSway</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-large-language-models-for-bio-image-analysis">Using Large Language Models for Bio-image Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-language-models-for-bio-image-data-science">Vision Language Models for Bio-image Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-abstractive-snippet-corpus-2020">Webis Abstractive Snippet Corpus 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-generated-native-ads-2024">Webis Generated Native Ads 2024</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-gmane-email-corpus-2019">Webis Gmane Email Corpus 2019</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-scsmeta-2021">Webis SCSmeta 2021</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-systematic-reviews-for-computer-science-2025">Webis Systematic Reviews for Computer Science 2025</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-trigger-warning-corpus-2023">Webis Trigger Warning Corpus 2023</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-context-sensitive-word-search-queries-2022">Webis-Context-sensitive-Word-Search-Queries-2022</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-generated-game-art-23">Webis-Generated-Game-Art-23</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-stereo-21">Webis-STEREO-21</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-stereo-21-full-version">Webis-STEREO-21 (Full Version)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-web-archive-17">Webis-Web-Archive-17</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wie-generative-ki-arbeit-und-studium-verandert">Wie Generative KI Arbeit und Studium verändert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wie-generative-ki-unsere-tagliche-arbeit-verandert">Wie Generative KI unsere tägliche Arbeit verändert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wie-generative-kunstliche-intelligenz-arbeit-und-studium-verandert">Wie Generative Künstliche Intelligenz Arbeit und Studium verändert</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="doi-org-52">
<h1><a class="reference external" href="http://Doi.org">Doi.org</a> (52)<a class="headerlink" href="#doi-org-52" title="Link to this heading">#</a></h1>
<section id="ai-4-se-exploring-hyperparameter-usage-and-tuning-in-machine-learning-research-cain-2023-submission">
<h2>AI-4-SE/Exploring-Hyperparameter-Usage-And-Tuning-In-Machine-Learning-Research: CAIN-2023 Submission<a class="headerlink" href="#ai-4-se-exploring-hyperparameter-usage-and-tuning-in-machine-learning-research-cain-2023-submission" title="Link to this heading">#</a></h2>
<p>Sebastian Simon</p>
<p>Published 2023-03-17</p>
<p>Licensed other-open</p>
<p>No description provided.</p>
<p><a class="reference external" href="https://zenodo.org/records/7745740">https://zenodo.org/records/7745740</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7745740">https://doi.org/10.5281/zenodo.7745740</a></p>
</section>
<hr class="docutils" />
<section id="ai4psychology-ki-kompetenz-training">
<h2>AI4Psychology KI-Kompetenz-Training<a class="headerlink" href="#ai4psychology-ki-kompetenz-training" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-09-25</p>
<p>Licensed cc-by-4.0</p>
<p>Dieses Slide-Deck is begleitend zum KI-Kompetenz-Training für Psychologinnen und Psychologen von <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> an der Uni Leipzig: <a class="reference external" href="https://scads.github.io/ai4psychology-2025/intro.html">https://scads.github.io/ai4psychology-2025/intro.html</a> 
Es umreißt die Themen</p>
<p>Einführung in Künstliche Intelligenz, KI-Systeme und Sprachmodelle</p>
<p>Anwendungsgebiete und Grenzen von KI bei der Text-Generierung</p>
<p>Prompt-Engineering</p>
<p>Prompten mit großem Kontext</p>
<p>Wissensdestillation</p>
<p>Embeddings und Retrieval-Augmented Generation</p>
<p>Bias</p>
<p>Datenschutzkonforme Nutzung</p>
<p>Urheberrecht</p>
<p>EU AI Act</p>
<p><a class="reference external" href="https://zenodo.org/records/17203260">https://zenodo.org/records/17203260</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17203260">https://doi.org/10.5281/zenodo.17203260</a></p>
</section>
<hr class="docutils" />
<section id="autonomous-ai-systems-for-data-analysis">
<h2>Autonomous AI Systems for Data Analysis<a class="headerlink" href="#autonomous-ai-systems-for-data-analysis" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-09-01</p>
<p>Licensed cc-by-4.0</p>
<p>In this slide deck we go through challenges towards building Autonomous AI Systems for Data Analysis. Along the path, we introduce prompt-engineering techiques aiming at improving the quality of AI-generated code.</p>
<p><a class="reference external" href="https://zenodo.org/records/17017890">https://zenodo.org/records/17017890</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17017890">https://doi.org/10.5281/zenodo.17017890</a></p>
</section>
<hr class="docutils" />
<section id="bio-image-data-science-lectures-2025-uni-leipzig-scads-ai">
<h2>Bio-image Data Science Lectures 2025 &#64; Uni Leipzig / <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a><a class="headerlink" href="#bio-image-data-science-lectures-2025-uni-leipzig-scads-ai" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-07-10</p>
<p>Licensed cc-by-4.0</p>
<p>These are the PPTx training resources for Students at Uni Leipzig who want to dive into bio-image data science with Python. The material will develop here and in the corresponding github repository between April and July 2025.</p>
<p><a class="reference external" href="https://zenodo.org/records/15858127">https://zenodo.org/records/15858127</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15858127">https://doi.org/10.5281/zenodo.15858127</a></p>
</section>
<hr class="docutils" />
<section id="data-literacy-open-science-and-collaborative-coding-and-ci-cd-slides-for-the-scads-ai-ga-workshop-working-with-data-and-code-like-a-pro">
<h2>Data literacy, Open Science and Collaborative Coding and CI/CD - Slides for the <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> GA-Workshop “Working with data and code like a pro”<a class="headerlink" href="#data-literacy-open-science-and-collaborative-coding-and-ci-cd-slides-for-the-scads-ai-ga-workshop-working-with-data-and-code-like-a-pro" title="Link to this heading">#</a></h2>
<p>Berger, Mike, Täschner, Matthias, Voigt, Pia</p>
<p>Published 2025-09-16</p>
<p>Licensed cc-by-4.0</p>
<p>This presentation contains introductions in data literacy, research data handling with an emphasise on Open Science as well as collaborative coding and CI/CD. These slides where presented in the workshop “Working with data and code like a pro” as part of the General Assembly at <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> on September 9th, 2025.  
The materials are intented for anyone interested in learning about research data handling and collaborative coding.</p>
<p><a class="reference external" href="https://zenodo.org/records/17130780">https://zenodo.org/records/17130780</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17130780">https://doi.org/10.5281/zenodo.17130780</a></p>
</section>
<hr class="docutils" />
<section id="devops-hands-on-training">
<h2>DevOps Hands-on Training<a class="headerlink" href="#devops-hands-on-training" title="Link to this heading">#</a></h2>
<p>Taeschner, Matthias, Haase, Robert</p>
<p>Published 2025-06-13</p>
<p>Licensed cc-by-4.0</p>
<p>In diesem Slide-Deck behandeln wir die Grundlagen von Versionskontrolle, kollaborativem Arbeiten mit Git, virtuelle Python Umgebungen, Continuous Integration und Continuous Delivery (CI/CD). </p>
<p><a class="reference external" href="https://zenodo.org/records/15654889">https://zenodo.org/records/15654889</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15654889">https://doi.org/10.5281/zenodo.15654889</a></p>
</section>
<hr class="docutils" />
<section id="emnlp-23-bootstrapping-a-violence-detector-for-fan-fiction">
<h2>EMNLP-23-Bootstrapping-a-Violence-Detector-for-Fan-Fiction<a class="headerlink" href="#emnlp-23-bootstrapping-a-violence-detector-for-fan-fiction" title="Link to this heading">#</a></h2>
<p>Wolska, Magdalena, Wiegmann, Matti, Schröder, Christopher, Borchardt, Ole, Stein, Benno, Potthast, Martin</p>
<p>Published 2023-10-24</p>
<p>Licensed cc-by-4.0</p>
<p>Data for the paper <code class="docutils literal notranslate"><span class="pre">Trigger</span> <span class="pre">Warnings:</span> <span class="pre">Bootstrapping</span> <span class="pre">a</span> <span class="pre">Violence</span> <span class="pre">Detector</span> <span class="pre">for</span> <span class="pre">Fan</span> <span class="pre">Fiction</span></code>. Code: <a class="github reference external" href="https://github.com/webis-de/emnlp23-bootstrapping-a-violence-detector-for-fan-fictionPublication:">webis-de/emnlp23-bootstrapping-a-violence-detector-for-fan-fictionPublication:</a> tbd. Citation: <a class="reference external" href="https://webis.de/publications.html?q=wolska_2023">https://webis.de/publications.html?q=wolska_2023</a> </p>
<p><a class="reference external" href="https://zenodo.org/records/10036479">https://zenodo.org/records/10036479</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10036479">https://doi.org/10.5281/zenodo.10036479</a></p>
</section>
<hr class="docutils" />
<section id="european-union-artificial-intelligence-act-in-a-nutshell">
<h2>European Union Artificial Intelligence Act (in a nutshell)<a class="headerlink" href="#european-union-artificial-intelligence-act-in-a-nutshell" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-06-19</p>
<p>Licensed cc-by-4.0</p>
<p>In this slide deck we introduce the European Artificial Intelligence Act (EU AI Act), the most important terms defined in it and major rules introduced by the act. We also take a quick look into related data protection and copyright regulations.</p>
<p><a class="reference external" href="https://zenodo.org/records/15699459">https://zenodo.org/records/15699459</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15699459">https://doi.org/10.5281/zenodo.15699459</a></p>
</section>
<hr class="docutils" />
<section id="explainable-machine-learning">
<h2>Explainable Machine Learning<a class="headerlink" href="#explainable-machine-learning" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-09-14</p>
<p>Licensed cc-by-4.0</p>
<p>In this lecture we will dive into Explainable Machine Learning, covering basic definition of the term and the field, including SHAP-Analysis and Grad-CAM as examples.
Related training materials are available online too: <a class="reference external" href="https://scads.github.io/ai4medicine-2025/day2.4_explainability/readme.html">https://scads.github.io/ai4medicine-2025/day2.4_explainability/readme.html</a></p>
<p><a class="reference external" href="https://zenodo.org/records/17116758">https://zenodo.org/records/17116758</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17116758">https://doi.org/10.5281/zenodo.17116758</a></p>
</section>
<hr class="docutils" />
<section id="how-generative-ai-impacts-research-and-teaching">
<h2>How Generative AI impacts research and teaching<a class="headerlink" href="#how-generative-ai-impacts-research-and-teaching" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-10-23</p>
<p>Licensed cc-by-4.0</p>
<p>In this Lunch Lecture at Leipzig University, we explore how Generative Artificial Intelligence (GenAI) impacts Reseaerch and Teaching. We dive into specialized GenAI-based web applications for scientific [literature] research and for generating teaching materials. There is a soft focus on pitfalls and potential ways to avoid them.</p>
<p><a class="reference external" href="https://zenodo.org/records/17422917">https://zenodo.org/records/17422917</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17422917">https://doi.org/10.5281/zenodo.17422917</a></p>
</section>
<hr class="docutils" />
<section id="how-llms-impact-bioimage-data-science">
<h2>How LLMs impact BioImage Data Science<a class="headerlink" href="#how-llms-impact-bioimage-data-science" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-11-21</p>
<p>Licensed cc-by-4.0</p>
<p>In this slide-deck we dive into large language models for bioimage data science, focusing on code generation for bio image analysis. The slides also give an outlook on how AI-systems may change data analysis in the future through data analysis code generation.</p>
<p><a class="reference external" href="https://zenodo.org/records/17669681">https://zenodo.org/records/17669681</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17669681">https://doi.org/10.5281/zenodo.17669681</a></p>
</section>
<hr class="docutils" />
<section id="how-large-language-models-impact-bio-image-data-science">
<h2>How Large Language Models impact Bio-Image Data Science<a class="headerlink" href="#how-large-language-models-impact-bio-image-data-science" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-09-11</p>
<p>Licensed cc-by-4.0</p>
<p>In this slide deck we dive into Large Language Models (LLMs) and Vision Language Models (VLMs) for Bio-image Analysis. We test their capabilities for object counting, pointing at objects and drawing bounding boxes around them. We also dive into VLMs deciding for segmentation algorithms and LLMs for code generation. We see bia-bob, git-bob and [alice] in action, which represent three generations of AI-based coding assistants analysing image data increasingly autonomously.</p>
<p><a class="reference external" href="https://zenodo.org/records/17102027">https://zenodo.org/records/17102027</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17102027">https://doi.org/10.5281/zenodo.17102027</a></p>
</section>
<hr class="docutils" />
<section id="ir-lab-leipzig-jena-summer-term-2023">
<h2>IR Lab Leipzig Jena Summer Term 2023<a class="headerlink" href="#ir-lab-leipzig-jena-summer-term-2023" title="Link to this heading">#</a></h2>
<p>Scells, Harrisen, Elstner, Theresa, Fröbe, Maik, Akiki, Christopher, Gienapp, Lukas, Reimer, Jan Heinrich, Hagen, Matthias, Potthast, Martin</p>
<p>Published 2024-02-07</p>
<p>Licensed cc-by-4.0</p>
<p>The Datasets for the Information Retrieval Course at the University Leipzig and Jena in Summer Term 2023
This repository contains resources coupled to ir_datasets and TIREx for IR courses that focus their hands-on labs on shared tasks. During the IR exercises in summer term 2023, we collaboratively developed and evaluated IR systems in a shared task style setup, covering corpus creation, system development, and statistical analysis. The resulting artifacts, i.e., the documents, topics, runs, relevance judgments can be browsed at <a class="reference external" href="https://tira-io.github.io/ir-lab-sose-23">https://tira-io.github.io/ir-lab-sose-23</a>. This zenodo artifact contains all of the underlying datasets used and produced during the course together with instructions on how to easily access the data using ir_datasets.
Overview of resources
This dataset contains the resources used and created during the IR course at <a class="reference external" href="https://temir.org/teaching/information-retrieval-ss23/information-retrieval-ss23.html">https://temir.org/teaching/information-retrieval-ss23/information-retrieval-ss23.html</a>.
The artifact contains the following files:</p>
<p> iranthology-20230618-training-inputs.zip containing the inputs to systems, i.e., containing the document corpus and the topics.
iranthology-20230618-training-truths.zip containing the truth to evaluate and tune systems, i.e., the topics and relevance judgments.</p>
<p> Accessing the Data with ir_datasets
We provide wrapper code to easily access the resources with ir_datasets:</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="this-loads-a-patched-version-of-ir-datasets-that-can-load-resources-from-tira">
<h1>this loads a patched version of ir_datasets that can load resources from TIRA<a class="headerlink" href="#this-loads-a-patched-version-of-ir-datasets-that-can-load-resources-from-tira" title="Link to this heading">#</a></h1>
<p>from tira.third_party_integrations import ir_datasets</p>
<p>dataset = ir_datasets.load(‘ir-lab-jena-leipzig-sose-2023/iranthology-20230618-training’)
Similarly, the same is possible with the ir_datasets integration to PyTerrier:
from tira.third_party_integrations import ensure_pyterrier_is_loaded
import pyterrier as pt</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="this-patches-ir-datasets-and-loads-pyterrier-so-that-it-can-load-resources-from-tira-and-can-run-in-the-tira-sandbox">
<h1>this patches ir_datasets and loads PyTerrier so that it can load resources from TIRA and can run in the TIRA sandbox<a class="headerlink" href="#this-patches-ir-datasets-and-loads-pyterrier-so-that-it-can-load-resources-from-tira-and-can-run-in-the-tira-sandbox" title="Link to this heading">#</a></h1>
<p>ensure_pyterrier_is_loaded()</p>
<p>dataset = pt.datasets.get_dataset(‘irds:ir-lab-jena-leipzig-sose-2023/iranthology-20230618-training’)</p>
<p><a class="reference external" href="https://zenodo.org/records/10628640">https://zenodo.org/records/10628640</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10628640">https://doi.org/10.5281/zenodo.10628640</a></p>
<hr class="docutils" />
<section id="ir-lab-leipzig-jena-winter-term-2023-2024">
<h2>IR Lab Leipzig/Jena Winter Term 2023/2024<a class="headerlink" href="#ir-lab-leipzig-jena-winter-term-2023-2024" title="Link to this heading">#</a></h2>
<p>Akiki, Christopher, Barschel, Hanno, Elstner, Theresa, Fleisch, Till, Franke, Johannes, Fröbe, Maik, Gienapp, Lukas, Gründel, Marlene, Hagen, Matthias, Kucera, Paul, Marschner, Paul, Niederhausen, Tim, Potthast, Martin, Reimer, Jan Heinrich, Scells, Harrisen, Stein, Benno, Weber, Malte, Wild, Dominic</p>
<p>Published 2024-02-07</p>
<p>Licensed cc-by-4.0</p>
<p>The Datasets for the Information Retrieval Course at the University Leipzig and Jena in Winter Term 2023/2024
This repository contains resources coupled to ir_datasets and TIREx for IR courses that focus their hands-on labs on shared tasks. During the IR exercises in winter term 2023/2024, we collaboratively developed and evaluated IR systems in a shared task style setup, covering corpus creation, system development, and statistical analysis. The resulting artifacts, i.e., the documents, topics, runs, relevance judgments can be browsed at <a class="reference external" href="https://tira-io.github.io/ir-lab-ws-23">https://tira-io.github.io/ir-lab-ws-23</a>. This zenodo artifact contains all of the underlying datasets used and produced during the course together with instructions on how to easily access the data using ir_datasets.
 Overview of resources
This dataset contains the resources used and created during the IR course at <a class="reference external" href="https://temir.org/teaching/information-retrieval-ws23/information-retrieval-ws23.html">https://temir.org/teaching/information-retrieval-ws23/information-retrieval-ws23.html</a>.
The artifact in this dataset include the following files:</p>
<p>training-20231104-training-inputs.zip containing the training inputs of LongEval 2023 to systems, i.e., containing the document corpus and the topics.
training-20231104-training-truths.zip containing the training truth of LongEval 2023 to evaluate and tune systems, i.e., the topics and relevance judgments.
validation-20231104-inputs.zip containing the validation inputs of LongEval 2023 to systems, i.e., containing the document corpus and the topics.
validation-20231104-truths.zip containing the validation truth of LongEval 2023 to evaluate and tune systems, i.e., the topics and relevance judgments.
jena-topics-small-20240119-inputs.zip containing the inputs to systems created in the course at the University of Jena, i.e., containing the document corpus and the topics.
jena-topics-small-20240119-training-truths.zip containing the truth to evaluate and tune systems created in the course at the University of Jena, i.e., the topics and relevance judgments.
leipzig-topics-small-20240119-training-inputs.zip containing the inputs to systems created in the course at the University of Leipzig, i.e., containing the document corpus and the topics.
leipzig-topics-small-20240119-training-truths.zip containing the truth to evaluate and tune systems created in the course at the University of Leipzig, i.e., the topics and relevance judgments.</p>
<p>Accessing the Data with ir_datasets
We provide wrapper code to easily access the resources with ir_datasets:</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>this loads a patched version of ir_datasets that can load resources from TIRA<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>from tira.third_party_integrations import ir_datasets</p>
<p>training_dataset = ir_datasets.load(‘ir-lab-jena-leipzig-wise-2023/training-20231104-training’)
validation_dataset = ir_datasets.load(‘ir-lab-jena-leipzig-wise-2023/validation-20231104-training’)
leipzig_dataset = ir_datasets.load(‘ir-lab-jena-leipzig-wise-2023/leipzig-topics-small-20240119-training’)
jena_dataset = ir_datasets.load(‘ir-lab-jena-leipzig-wise-2023/jena-topics-small-20240119-training’)
Similarly, the same is possible with the ir_datasets integration to PyTerrier:
from tira.third_party_integrations import ensure_pyterrier_is_loaded
import pyterrier as pt</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id2">
<h1>this patches ir_datasets and loads PyTerrier so that it can load resources from TIRA and can run in the TIRA sandbox<a class="headerlink" href="#id2" title="Link to this heading">#</a></h1>
<p>ensure_pyterrier_is_loaded()</p>
<p>training_dataset = pt.datasets.get_dataset(‘irds:ir-lab-jena-leipzig-wise-2023/training-20231104-training’)
validation_dataset = pt.datasets.get_dataset(‘irds:ir-lab-jena-leipzig-wise-2023/validation-20231104-training’)
leipzig_dataset = pt.datasets.get_dataset(‘irds:ir-lab-jena-leipzig-wise-2023/leipzig-topics-small-20240119-training’)
jena_dataset = pt.datasets.get_dataset(‘irds:ir-lab-jena-leipzig-wise-2023/jena-topics-small-20240119-training’)
 
License
The dataset is derived from the <a class="reference external" href="https://clef-longeval.github.io/">LongEval 2024</a> shared task hosted at <a class="reference internal" href="#"><span class="xref myst">CLEF 2024</span></a>.We use the LongEval documents, therefore, if you use this resource, please cite the <a class="reference external" href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5151">corresponding dataset</a>:
&#64;misc{11234/1-5151,
title = {{LongEval} Click-Model Relevance Judgements (Qrels)},
author = {Galu{\v s}{\v c}{‘a}kov{‘a}, Petra and Devaud, Romain and Gonzalez-Saez, Gabriela and Mulhem, Philippe and Goeuriot, Lorraine and Piroi, Florina and Popel, Martin},
url = {<a class="reference external" href="http://hdl.handle.net/11234/1-5151">http://hdl.handle.net/11234/1-5151</a>},
note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{‘U}FAL}), Faculty of Mathematics and Physics, Charles University},
copyright = {Qwant {LongEval} Attribution-{NonCommercial}-{ShareAlike} License},
year = {2023} }
Furthermore, because we use the LongEval documents, this resource is under the Qwant LongEval Attribution-NonCommercial-ShareAlike License and by reusing this resource you also accept and aggree to do this under the sharealike qwant license.</p>
<p><a class="reference external" href="https://zenodo.org/records/10628882">https://zenodo.org/records/10628882</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10628882">https://doi.org/10.5281/zenodo.10628882</a></p>
<hr class="docutils" />
<section id="ki-kompetenz-training">
<h2>KI-Kompetenz-Training<a class="headerlink" href="#ki-kompetenz-training" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-05-26</p>
<p>Licensed cc-by-4.0</p>
<p>Dieses Slide-Deck is begleitend zu dem KI-Kompetenz-Training von <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> an der Uni Leipzig: <a class="reference external" href="https://scads.github.io/ki-kompetenz-training-2025/intro.html">https://scads.github.io/ki-kompetenz-training-2025/intro.html</a> 
Es umreißt die Themen</p>
<p>Einführung in Künstliche Intelligenz, KI-Systeme und Sprachmodelle</p>
<p>Anwendungsgebiete und Grenzen von KI bei der Text-Generierung</p>
<p>Prompt-Engineering</p>
<p>Prompten mit großem Kontext</p>
<p>Wissensdestillation</p>
<p>Embeddings und Retrieval-Augmented Generation</p>
<p>Datenanalyse</p>
<p>Daten- und Code-Generierung</p>
<p>Bias</p>
<p>Datenschutzkonforme Nutzung</p>
<p>Urheberrecht</p>
<p>EU AI Act</p>
<p><a class="reference external" href="https://zenodo.org/records/15516022">https://zenodo.org/records/15516022</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15516022">https://doi.org/10.5281/zenodo.15516022</a></p>
</section>
<hr class="docutils" />
<section id="manipulating-embeddings-of-stable-diffusion-prompts">
<h2>Manipulating Embeddings of Stable Diffusion Prompts<a class="headerlink" href="#manipulating-embeddings-of-stable-diffusion-prompts" title="Link to this heading">#</a></h2>
<p>Deckers, Niklas, Peters, Julia, Potthast, Martin</p>
<p>Published 2024-05-16</p>
<p>Licensed cc-by-4.0</p>
<p>Supplementary material of the paper Manipulating Embeddings of Stable Diffusion Prompts.
The paper can be found in the proceedings: IJCAI 2024 Proceedings
Please cite as:
&#64;InProceedings{deckers:2024a,  author =                   {Niklas Deckers and Julia Peters and Martin Potthast},  booktitle =                {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}},  doi =                      {10.24963/ijcai.2024/845},  editor =                   {Kate Larson},  month =                    aug,  pages =                    {7636–7644},  publisher =                {International Joint Conferences on Artificial Intelligence Organization},  title =                    ,  url =                      {<a class="reference external" href="https://doi.org/10.24963/ijcai.2024/845">https://doi.org/10.24963/ijcai.2024/845</a>},  year =                     2024}
 </p>
<p><a class="reference external" href="https://zenodo.org/records/11203881">https://zenodo.org/records/11203881</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11203881">https://doi.org/10.5281/zenodo.11203881</a></p>
</section>
<hr class="docutils" />
<section id="pan23-trigger-detection">
<h2>PAN23 Trigger Detection<a class="headerlink" href="#pan23-trigger-detection" title="Link to this heading">#</a></h2>
<p>Matti Wiegmann, Magdalena Wolska, Christopher Schröder, Ole Borchardt, Benno Stein, Martin Potthast</p>
<p>Published 2023-02-02</p>
<p>This is the dataset for the shared task on Trigger Detection at PAN&#64;CLEF2023. Please consult the task’s page for further details on the format, the dataset’s creation, and links to baselines and utility code.</p>
<p>You can find a more refined version of this work here: <a class="reference external" href="http://github.com/webis-de/ACL-23">github.com/webis-de/ACL-23</a>.</p>
<p>Task: In trigger detection, we want to assign trigger warning labels to documents that contain potentially discomforting or distressing (triggering) content. We model trigger detection as a multi-label document classification challenge: assign each document all appropriate trigger warnings, but not more. All warnings are chosen from the author’s perspective, i.e. the work’s author decided which kind of trigger the document contains.</p>
<p>Dataset: This dataset contains annotated works of fanfiction, extracted from <a class="reference external" href="http://archiveofourown.org">archiveofourown.org</a> (AO3). Each work is between 50 and 6,000 words long and has between 1 and many trigger warnings assigned. Our training dataset contains 307,102 examples, with 17,104 in validation and 17,040 in the test split. The label set contains 32 different trigger warnings. All labels are based on the freeform content warnings added to a fanwork by its author.</p>
<p>Versioning: </p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1.0: initial upload
1.1 fixed a minor bug where some works in the labels.jsonl contained labels that are not used in the competition (heteronormativity and religious-discrimination). Those labels have been removed.&amp;nbsp;&amp;nbsp;
1.2 added labels.jsonl for the test dataset.
</pre></div>
</div>
<p><a class="reference external" href="https://zenodo.org/records/8383863">https://zenodo.org/records/8383863</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8383863">https://doi.org/10.5281/zenodo.8383863</a></p>
</section>
<hr class="docutils" />
<section id="prompt-engineering-techniques-slides-for-the-scads-ai-ga-workshop-coding-effectively-with-ai-getting-started-with-cursor-and-copilot">
<h2>Prompt Engineering Techniques - Slides for the <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> GA-Workshop “Coding effectively with AI: Getting started with Cursor and Copilot”<a class="headerlink" href="#prompt-engineering-techniques-slides-for-the-scads-ai-ga-workshop-coding-effectively-with-ai-getting-started-with-cursor-and-copilot" title="Link to this heading">#</a></h2>
<p>Kabjesz, Lea, Gihlein, Lea, Lampert, Mara Harriet, Götze, Luisa</p>
<p>Published 2025-09-10</p>
<p>Licensed cc-by-4.0</p>
<p>This record contains the PDF and PPTX Slides that were used at the <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> Leipzig/Dresden General-Assembly Workshop “Coding effectively with AI: Getting started with Cursor and Copilot”, held on 9th September 2025. The slides and related materials are also avaialbe on the corresponding GitHub repository. 
The materials are intented for anyone interested in learning about prompt engineering techniques and AI-assisted coding with Cursor. </p>
<p><a class="reference external" href="https://zenodo.org/records/17093197">https://zenodo.org/records/17093197</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17093197">https://doi.org/10.5281/zenodo.17093197</a></p>
</section>
<hr class="docutils" />
<section id="reproducible-bio-image-analysis-using-python-napari-jupyter-and-ai">
<h2>Reproducible Bio-Image Analysis using Python, Napari, Jupyter and AI<a class="headerlink" href="#reproducible-bio-image-analysis-using-python-napari-jupyter-and-ai" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-09-09</p>
<p>Licensed cc-by-4.0</p>
<p>In this slide deck we learn how to write reproducible bio-image analysis code in Jupyter notebooks. Goal is not just to have code running elsewhere reproducibly, but also enabling others to understand workflows to enable them reproducing the analysis also in their mind and potentially other tools. Additionally we cover how to generate Jupyter notebooks from Napari and using artificial intelligence, namely bia-bob.</p>
<p><a class="reference external" href="https://zenodo.org/records/17085991">https://zenodo.org/records/17085991</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17085991">https://doi.org/10.5281/zenodo.17085991</a></p>
</section>
<hr class="docutils" />
<section id="research-data-management">
<h2>Research Data Management<a class="headerlink" href="#research-data-management" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-09-23</p>
<p>Licensed cc-by-4.0</p>
<p>This slides covers aspects of research data management (RDM) and open science such as the RDM life cylce, FAIR principles, sharing data on Zenodo, rights and duties of scientists in the RDM context.</p>
<p><a class="reference external" href="https://zenodo.org/records/17186869">https://zenodo.org/records/17186869</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17186869">https://doi.org/10.5281/zenodo.17186869</a></p>
</section>
<hr class="docutils" />
<section id="same-sentiment-classification-train-dev-test-pair-ids">
<h2>Same Sentiment Classification Train/Dev/Test Pair IDs<a class="headerlink" href="#same-sentiment-classification-train-dev-test-pair-ids" title="Link to this heading">#</a></h2>
<p>Erik Körner, Ahmad Dawar Hakimi, Gerhard Heyer, Martin Potthast</p>
<p>Published 2021-09-09</p>
<p>Licensed cc-by-4.0</p>
<p>This “dataset” only includes the compiled pairings of the Yelp Business Review Dataset. To get access to the actual review texts, please follow the instructions on the Yelp Dataset webpage.</p>
<p>The data format is JSONlines.
Python Load Example:</p>
<p>import pandas as pd
traindev_df = pd.read_json(“df_traindev.jsonl”, lines=True)
test_df = pd.read_json(“df_test.jsonl”, lines=True)</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="example-access-to-single-business-review-id">
<h1>example access to single business/review id<a class="headerlink" href="#example-access-to-single-business-review-id" title="Link to this heading">#</a></h1>
<p>s1_bid = test_df.iloc[0][“sent1_business_id”]
s1_rid = test_df.iloc[0][“sent1_review_id”]
s2_bid = test_df.iloc[0][“sent2_business_id”]
s2_rid = test_df.iloc[0][“sent2_review_id”]
label = test_df.iloc[0][“is_same_side”]</p>
<p>See documentation at:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Yelp Dataset Schemata (only business.json and review.json were used)
Yelp Business Category Hierarchy (download the json file as all_category_list.json)
</pre></div>
</div>
<p>For details on how the data was compiled and used in our experiments, please refer to our code repository. Other derived data splits can be reproduced deterministically by using the same random seed as in our experiments.</p>
<p><a class="reference external" href="https://zenodo.org/records/5495793">https://zenodo.org/records/5495793</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5495793">https://doi.org/10.5281/zenodo.5495793</a></p>
<hr class="docutils" />
<section id="same-side-stance-classification-adversarial-test-cases">
<h2>Same Side Stance Classification Adversarial Test Cases<a class="headerlink" href="#same-side-stance-classification-adversarial-test-cases" title="Link to this heading">#</a></h2>
<p>Ahmad Dawar Hakimi, Erik Körner, Gregor Wiedemann, Gerhard Heyer, Martin Potthast</p>
<p>Published 2021-09-09</p>
<p>Licensed cc-by-4.0</p>
<p>This dataset contains 175 cases. It is manually created to reveal the ability of models to solve different types of adversarial cases for same side stance predictions more systematically. The examples selected here are derived from the dataset used in the Same Side Stance Classification shared task.</p>
<p>We have selected 25 distinct arguments from the “gay marriage” topic that are short and express their stance clearly. For each selected argument, we construct new arguments of four distinct types to obtain two pairs, one with the same stance, and one with an opposing stance:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Negation: a simple negation of the argument.
Paraphrase: alters important words from the argument to synonymous expressions with the same stance.
Argument: uses an argument from the same topic and stance, but semantically completely different regarding the first one.
Citation: repeats or summarizes the first argument and then expresses agreement or rejection (a case frequently occurring in the dataset).
</pre></div>
</div>
<p>The types Paraphrase, Argument, and Citation are also formulated in a negated version to create additional test instances for the opposite stance.</p>
<p><a class="reference external" href="https://zenodo.org/records/5282635">https://zenodo.org/records/5282635</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5282635">https://doi.org/10.5281/zenodo.5282635</a></p>
</section>
<hr class="docutils" />
<section id="same-side-stance-classification-resampled-datasets">
<h2>Same Side Stance Classification Resampled Datasets<a class="headerlink" href="#same-side-stance-classification-resampled-datasets" title="Link to this heading">#</a></h2>
<p>Gregor Wiedemann, Erik Körner, Ahmad Dawar Hakimi, Gerhard Heyer, Martin Potthast</p>
<p>Published 2021-09-09</p>
<p>Licensed cc-by-4.0</p>
<p>The resampled datasets for the Same Side Stance Classification problem used in the EMNLP’21 paper “On Classifying whether Two Texts are on the Same Side of an Argument”.</p>
<p>The data is based on the publicly available S3C training datasets.</p>
<p>The data format is JSONlines.
Python Load Example: (for every single task split)</p>
<p>import pandas as pd
df_cross_dev = pd.read_json(“cross_dev.jsonl”, lines=True)</p>
<p>For details on how the data was compiled, please refer to our code.</p>
<p><a class="reference external" href="https://zenodo.org/records/5380989">https://zenodo.org/records/5380989</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5380989">https://doi.org/10.5281/zenodo.5380989</a></p>
</section>
<hr class="docutils" />
<section id="scads-ai-meetup-4-2025-ai-insights-on-ai-agents-agent-frameworks">
<h2><a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> Meetup #4 2025: AI Insights on AI Agents &amp; Agent-Frameworks<a class="headerlink" href="#scads-ai-meetup-4-2025-ai-insights-on-ai-agents-agent-frameworks" title="Link to this heading">#</a></h2>
<p>Center for Scalable Data Analytics and Artificial Intelligence, Oliver, Welz</p>
<p>Published 2025-05-20</p>
<p>Licensed cc-by-4.0</p>
<p>Presentation on the AI insights regarding AI-Agents and Agent-Frameworks at the Meetup #4 2025 at <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a>.
The presentation covers defintions of (AI) Agents as well as a historical &amp; technical classification of the term (AI) Agent.</p>
<p><a class="reference external" href="https://zenodo.org/records/15805148">https://zenodo.org/records/15805148</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15805148">https://doi.org/10.5281/zenodo.15805148</a></p>
</section>
<hr class="docutils" />
<section id="scads-ai-meetup-4-2025-workshop-ai-agents">
<h2><a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> Meetup #4 2025: Workshop AI Agents<a class="headerlink" href="#scads-ai-meetup-4-2025-workshop-ai-agents" title="Link to this heading">#</a></h2>
<p>Philipp, Schott, Gregor, Wolf, Oliver, Welz</p>
<p>Published 2025-05-20</p>
<p>Licensed cc-by-4.0</p>
<p>Workshop materials on the topic of AI-Agents at Meetup #4 2025 on <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a>.
Run &amp; learn about AI-Agents via low/no-code UI (Langflow) or via OpenAI Agents SDK.</p>
<p><a class="reference external" href="https://zenodo.org/records/15805268">https://zenodo.org/records/15805268</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15805268">https://doi.org/10.5281/zenodo.15805268</a></p>
</section>
<hr class="docutils" />
<section id="scads-ai-meetup-8-2025-ai-insights-model-context-protocol-mcp">
<h2><a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> Meetup #8 2025: AI Insights Model Context Protocol (MCP)<a class="headerlink" href="#scads-ai-meetup-8-2025-ai-insights-model-context-protocol-mcp" title="Link to this heading">#</a></h2>
<p>Welz, Oliver</p>
<p>Published 2025-09-19</p>
<p>Licensed cc-by-4.0</p>
<p>Presentation on the AI insights regarding Model Context Protocol (MCP) at the Meetup #8 2025 at <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a>.
The presentation covers the definition of MCP, a technical overview and overview of the architecture, as well as advantages and disadvantages, critical analysis, and added value for different user groups (developers, users).</p>
<p><a class="reference external" href="https://zenodo.org/records/17158643">https://zenodo.org/records/17158643</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17158643">https://doi.org/10.5281/zenodo.17158643</a></p>
</section>
<hr class="docutils" />
<section id="scads-ai-meetup-8-2025-workshop-model-context-protocol-mcp">
<h2><a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a> Meetup #8 2025: Workshop Model Context Protocol (MCP)<a class="headerlink" href="#scads-ai-meetup-8-2025-workshop-model-context-protocol-mcp" title="Link to this heading">#</a></h2>
<p>Schott, Philipp, Wolf, Gregor, Welz, Oliver</p>
<p>Published 2025-09-19</p>
<p>Licensed cc-by-4.0</p>
<p>Workshop materials on the topic of MCP at Meetup #8 2025 on <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a>.
Learn about MCP, how to add MCP Servers, use them e.g. via low/no-code UI (Langflow) or write your own MCP Servers with the FastMCP Python Package.</p>
<p><a class="reference external" href="https://zenodo.org/records/17158770">https://zenodo.org/records/17158770</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17158770">https://doi.org/10.5281/zenodo.17158770</a></p>
</section>
<hr class="docutils" />
<section id="selected-meta-data-of-accepted-contributions-to-some-ai-ml-conferences-2020-2025">
<h2>Selected meta data of accepted contributions to some AI/ML conferences 2020-2025<a class="headerlink" href="#selected-meta-data-of-accepted-contributions-to-some-ai-ml-conferences-2020-2025" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-10-29</p>
<p>Licensed cc-zero</p>
<p>This dataset contains author names, titles and urls of  accepted contributions to some AI/ML confereences (AAAI, ICML, NeurIPS and IJCAI) in the years 2020-2025.
The data originates from these sources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://icml.cc/Downloads/2020">https://icml.cc/Downloads/2020</a>* <a class="reference external" href="https://icml.cc/Downloads/2021">https://icml.cc/Downloads/2021</a>* <a class="reference external" href="https://icml.cc/Downloads/2022">https://icml.cc/Downloads/2022</a>* <a class="reference external" href="https://icml.cc/Downloads/2023">https://icml.cc/Downloads/2023</a>* <a class="reference external" href="https://icml.cc/Downloads/2024">https://icml.cc/Downloads/2024</a>* <a class="reference external" href="https://icml.cc/Downloads/2025">https://icml.cc/Downloads/2025</a>* <a class="reference external" href="https://aaai.org/proceeding/aaai-34-2020/">https://aaai.org/proceeding/aaai-34-2020/</a>* <a class="reference external" href="https://aaai.org/proceeding/aaai-35-2021/">https://aaai.org/proceeding/aaai-35-2021/</a>* <a class="reference external" href="https://aaai.org/proceeding/aaai-36-2022/">https://aaai.org/proceeding/aaai-36-2022/</a>* <a class="reference external" href="https://aaai.org/proceeding/aaai-37-2023/">https://aaai.org/proceeding/aaai-37-2023/</a>* <a class="reference external" href="https://aaai.org/proceeding/aaai-38-2024/">https://aaai.org/proceeding/aaai-38-2024/</a>* <a class="reference external" href="https://aaai.org/proceeding/aaai-39-2025/">https://aaai.org/proceeding/aaai-39-2025/</a>* <a class="reference external" href="https://www.ijcai.org/proceedings/2020/">https://www.ijcai.org/proceedings/2020/</a>* <a class="reference external" href="https://www.ijcai.org/proceedings/2021/">https://www.ijcai.org/proceedings/2021/</a>* <a class="reference external" href="https://www.ijcai.org/proceedings/2022/">https://www.ijcai.org/proceedings/2022/</a>* <a class="reference external" href="https://www.ijcai.org/proceedings/2023/">https://www.ijcai.org/proceedings/2023/</a>* <a class="reference external" href="https://www.ijcai.org/proceedings/2024/">https://www.ijcai.org/proceedings/2024/</a>* <a class="reference external" href="https://www.ijcai.org/proceedings/2025/">https://www.ijcai.org/proceedings/2025/</a>* <a class="reference external" href="https://neurips.cc/Downloads/2020">https://neurips.cc/Downloads/2020</a>* <a class="reference external" href="https://neurips.cc/Downloads/2021">https://neurips.cc/Downloads/2021</a>* <a class="reference external" href="https://neurips.cc/Downloads/2022">https://neurips.cc/Downloads/2022</a>* <a class="reference external" href="https://neurips.cc/Downloads/2023">https://neurips.cc/Downloads/2023</a>* <a class="reference external" href="https://neurips.cc/Downloads/2024">https://neurips.cc/Downloads/2024</a>* <a class="reference external" href="https://neurips.cc/Downloads/2025">https://neurips.cc/Downloads/2025</a>
The data contains above mentioned meta data only, and no abstract, text, pdfs or anything else.</p></li>
</ul>
<p><a class="reference external" href="https://zenodo.org/records/17478160">https://zenodo.org/records/17478160</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17478160">https://doi.org/10.5281/zenodo.17478160</a></p>
</section>
<hr class="docutils" />
<section id="small-text-active-learning-for-text-classification-in-python">
<h2>Small-Text: Active Learning for Text Classification in Python<a class="headerlink" href="#small-text-active-learning-for-text-classification-in-python" title="Link to this heading">#</a></h2>
<p>Schröder, Christopher, Müller, Lydia, Niekler, Andreas, Potthast, Martin</p>
<p>Published 2025-08-17</p>
<p>Licensed mit-license</p>
<p>We present small-text, an easy-to-use active learning library, which offers pool-based active learning for single- and multi-label text classification in Python. It features many pre-implemented state-of-the-art query strategies, including some that leverage the GPU. Standardized interfaces allow the combination of a variety of classifiers, query strategies, and stopping criteria, facilitating a quick mix and match, and enabling a rapid development of both active learning experiments and applications. To make various classifiers and query strategies accessible in a unified way, small-text integrates the well-known machine learning libraries scikit-learn, PyTorch, and huggingface transformers. The latter integrations are available as optionally installable extensions, making the availability of a GPU competely optional. The library is publicly available under the MIT License at <a class="github reference external" href="https://github.com/webis-de/small-text">webis-de/small-text</a>.</p>
<p><a class="reference external" href="https://zenodo.org/records/16890132">https://zenodo.org/records/16890132</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.16890132">https://doi.org/10.5281/zenodo.16890132</a></p>
</section>
<hr class="docutils" />
<section id="smart-slide-generation-re-using-training-materials-through-the-power-of-llms">
<h2>Smart Slide Generation: Re-Using Training Materials Through the Power of LLMs<a class="headerlink" href="#smart-slide-generation-re-using-training-materials-through-the-power-of-llms" title="Link to this heading">#</a></h2>
<p>Kabjesz, Lea, Gihlein, Lea, Lampert, Mara Harriet, Haase, Robert</p>
<p>Published 2025-11-06</p>
<p>Licensed cc-by-4.0</p>
<p>This record contains the poster “Smart Slide Generation: Re-Using Training Materials Through the Power of LLMs” presented by Lea Kabjesz and Lea Gihlein at the All Hands Meeting of the German AI Centers, held on 5th - 6th November 2025 at DFKI Saarbrücken, Germany. 
 
We acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the National Research Data Infrastructure – NFDI 46/1 – 501864659.</p>
<p><a class="reference external" href="https://zenodo.org/records/17541532">https://zenodo.org/records/17541532</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17541532">https://doi.org/10.5281/zenodo.17541532</a></p>
</section>
<hr class="docutils" />
<section id="socio-technical-anti-patterns-in-building-ml-enabled-software-supplementary-material">
<h2>Socio-Technical Anti-Patterns in Building ML-Enabled Software (Supplementary Material)<a class="headerlink" href="#socio-technical-anti-patterns-in-building-ml-enabled-software-supplementary-material" title="Link to this heading">#</a></h2>
<p>Mailach, Alina, Siegmund, Norbert</p>
<p>Published 2023-01-18</p>
<p>Licensed cc-by-4.0</p>
<p>This material documents the coding process and theme evolution for the reflexive thematic analysis conducted in the paper  “Socio-Technical Anti-Patterns in Building ML-Enabled Software”, published at International Conference on Software Engineering, 2023. The provided codes and themes represent an intermediate stage of our work, where the file  initial_themes.pdf holds initial themes and codes and interim_and_final_themes.xlsx depicts more evolved themes and codes as well as a tracing of which codes belong to which themes. The themes developed over time and represent development contexts and activities, containing anti-patterns, causes, and recommendations. We had not fully developed anti-patterns during the coding process, but rather used symptoms.  </p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/7520777">https://zenodo.org/records/7520777</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7520777">https://doi.org/10.5281/zenodo.7520777</a></p>
</section>
<hr class="docutils" />
<section id="touche-25-advertisement-in-retrieval-augmented-generation">
<h2>Touché-25-Advertisement-in-Retrieval-Augmented-Generation<a class="headerlink" href="#touche-25-advertisement-in-retrieval-augmented-generation" title="Link to this heading">#</a></h2>
<p>Heineking, Sebastian, Zelch, Ines, Potthast, Martin, Hagen, Matthias</p>
<p>Published 2025-05-06</p>
<p>Licensed cc-by-4.0</p>
<p>Dataset for Sub-Task 1 (Generation) of the Touché 2025 Task 4. The goal of this task is to research advertisements in retrieval augmented generation (RAG). Towards this goal, the dataset provides queries from the Webis Generated Native Ads 2024 dataset and corresponding document segments from the segmented version of MS MARCO V2.1.</p>
<p><a class="reference external" href="https://zenodo.org/records/15347992">https://zenodo.org/records/15347992</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15347992">https://doi.org/10.5281/zenodo.15347992</a></p>
</section>
<hr class="docutils" />
<section id="touche20-argument-retrieval-for-controversial-questions">
<h2>Touché20-Argument-Retrieval-for-Controversial-Questions<a class="headerlink" href="#touche20-argument-retrieval-for-controversial-questions" title="Link to this heading">#</a></h2>
<p>Potthast, Martin, Gienapp, Lukas, Wachsmuth, Henning, Hagen, Matthias, Fröbe, Maik, Bondarenko, Alexander, Ajjour, Yamen, Stein, Benno</p>
<p>Published 2020-09-23</p>
<p>Licensed cc-by-4.0</p>
<p>Data for the Argument Retrieval for Controversial Questions task at Touché 2020.</p>
<p><a class="reference external" href="https://zenodo.org/records/6873564">https://zenodo.org/records/6873564</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6873564">https://doi.org/10.5281/zenodo.6873564</a></p>
</section>
<hr class="docutils" />
<section id="touche21-argument-retrieval-for-controversial-questions">
<h2>Touché21-Argument-Retrieval-for-Controversial-Questions<a class="headerlink" href="#touche21-argument-retrieval-for-controversial-questions" title="Link to this heading">#</a></h2>
<p>Gienapp, Lukas, Potthast, Martin, Wachsmuth, Henning, Hagen, Matthias, Fröbe, Maik, Bondarenko, Alexander, Ajjour, Yamen, Stein, Benno</p>
<p>Published 2024-09-30</p>
<p>Licensed cc-by-4.0</p>
<p>Data for the Argument Retrieval for Controversial Questions task at Touché 2021.</p>
<p><a class="reference external" href="https://zenodo.org/records/13860524">https://zenodo.org/records/13860524</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13860524">https://doi.org/10.5281/zenodo.13860524</a></p>
</section>
<hr class="docutils" />
<section id="touche22-image-retrieval-for-arguments">
<h2>Touché22-Image-Retrieval-for-Arguments<a class="headerlink" href="#touche22-image-retrieval-for-arguments" title="Link to this heading">#</a></h2>
<p>Kiesel, Johannes, Potthast, Martin, Stein, Benno</p>
<p>Published 2022-06-13</p>
<p>Licensed cc-by-4.0</p>
<p>Data for the Image Retrieval for Arguments task at Touché 2022.</p>
<p>This version is lacking the touche22-image-search-archives.zip and touche22-image-search-screenshots.zip for space restrictions. Please get them from <a class="reference external" href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/">https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/</a></p>
<p><a class="reference external" href="https://zenodo.org/records/6873575">https://zenodo.org/records/6873575</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6873575">https://doi.org/10.5281/zenodo.6873575</a></p>
</section>
<hr class="docutils" />
<section id="treesway">
<h2>TreeSway<a class="headerlink" href="#treesway" title="Link to this heading">#</a></h2>
<p>Umlauft, Josefine</p>
<p>Published 2025-10-01</p>
<p>Licensed cc-by-4.0</p>
<p>This dataset contains seismic and meteorological measurements from a field experiment on tree sway dynamics at the ECOSENSE site in 2024/2025. Ground-based seismometers and accelerometers on trees recorded responses to wind forcing, complemented by wind velocity data. The dataset supports the analyses in the manuscript ‘The Seismic Fingerprint of Wind-Induced Tree Sway’ (submitted, 2025). Data are provided as time series and derived spectral features, with gap intervals documented. See README.txt for details on format and usage.</p>
<p><a class="reference external" href="https://zenodo.org/records/17240205">https://zenodo.org/records/17240205</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17240205">https://doi.org/10.5281/zenodo.17240205</a></p>
</section>
<hr class="docutils" />
<section id="using-large-language-models-for-bio-image-analysis">
<h2>Using Large Language Models for Bio-image Analysis<a class="headerlink" href="#using-large-language-models-for-bio-image-analysis" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-09-04</p>
<p>Licensed cc-by-4.0</p>
<p>In this slide-deck we learn how prompt large language models for bio-image analysis code and how to do this in Jupyter notebooks using bia-bob, a tool integrated in jupyter notebooks for AI-assisted code editing and notebook generation.
 </p>
<p><a class="reference external" href="https://zenodo.org/records/17053832">https://zenodo.org/records/17053832</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17053832">https://doi.org/10.5281/zenodo.17053832</a></p>
</section>
<hr class="docutils" />
<section id="vision-language-models-for-bio-image-data-science">
<h2>Vision Language Models for Bio-image Data Science<a class="headerlink" href="#vision-language-models-for-bio-image-data-science" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-06-25</p>
<p>Licensed cc-by-4.0</p>
<p>In this talk, I demonstrate potential use-cases for vision-language models (VLM) in bio-image data science, focusing on how to analyse microscopy image data. It covers these use-cases:</p>
<p>cell counting
bounding-box segmentation
image descriptions
VLMs guessing which algorithm to use for processing
Data analysis code generation
Answering github issues </p>
<p>The talk also points at a number of VLM-based open-source tools which start reshaping the scientific bio-image data science domain:</p>
<p>bia-bob
unprompted
git-bob
napari-chatgpt
<a class="reference external" href="http://bioimage.io">bioimage.io</a> chatbot</p>
<p><a class="reference external" href="https://zenodo.org/records/15735577">https://zenodo.org/records/15735577</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15735577">https://doi.org/10.5281/zenodo.15735577</a></p>
</section>
<hr class="docutils" />
<section id="webis-abstractive-snippet-corpus-2020">
<h2>Webis Abstractive Snippet Corpus 2020<a class="headerlink" href="#webis-abstractive-snippet-corpus-2020" title="Link to this heading">#</a></h2>
<p>Chen ,Wei-Fan, Syed, Shahbaz, Potthast, Martin, Hagen, Matthias, Stein, Benno</p>
<p>Published 2020-02-07</p>
<p>Licensed cc-by-4.0</p>
<p>The Webis Abstractive Snippet 2020 (Webis-Snippete-20) comprises four abstractive snippet dataset from ClueWeb09, Clueweb12, and DMOZ descriptions. More than 10 million &lt;webpage, abstractive snippet&gt; pairs / 3.5 million &lt;query, webpage, abstractive snippet&gt; pairs were collected.</p>
<p><a class="reference external" href="https://zenodo.org/records/3653834">https://zenodo.org/records/3653834</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3653834">https://doi.org/10.5281/zenodo.3653834</a></p>
</section>
<hr class="docutils" />
<section id="webis-generated-native-ads-2024">
<h2>Webis Generated Native Ads 2024<a class="headerlink" href="#webis-generated-native-ads-2024" title="Link to this heading">#</a></h2>
<p>Heineking, Sebastian, Zelch, Ines, Bevendorff, Janek, Stein, Benno, Hagen, Matthias, Potthast, Martin</p>
<p>Published 2025-04-23</p>
<p>Licensed cc-by-4.0</p>
<p>Version of the Webis Generated Native Ads 2024 dataset prepared for Sub-Task 2 of the Advertisement in Retrieval-Augmented Generation task at Touché 2025.
The dataset contains the same data but split into JSONL-files and with separate files for responses/sentence pairs and labels.
Citation
&#64;InProceedings{schmidt:2024,  author =                   {Sebastian Schmidt and Ines Zelch and Janek Bevendorff and Benno Stein and Matthias Hagen and Martin Potthast},  booktitle =                {WWW ‘24: Proceedings of the ACM Web Conference 2024},  doi =                      {10.1145/3589335.3651489},  publisher =                {ACM},  site =                     {Singapore, Singapore},  title =                    ,  year =                     2024}</p>
<p><a class="reference external" href="https://zenodo.org/records/15270283">https://zenodo.org/records/15270283</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15270283">https://doi.org/10.5281/zenodo.15270283</a></p>
</section>
<hr class="docutils" />
<section id="webis-gmane-email-corpus-2019">
<h2>Webis Gmane Email Corpus 2019<a class="headerlink" href="#webis-gmane-email-corpus-2019" title="Link to this heading">#</a></h2>
<p>Janek Bevendorff, Khalid Al-Khatib, Martin Potthast, Benno Stein</p>
<p>Published 2020-06-03</p>
<p>The Webis Gmane Email Corpus 2019 is a dataset of more than 153 million parsed and segmented emails crawled between February and May 2019 from <a class="reference external" href="http://gmane.io">gmane.io</a> covering more than 20 years of public mailing lists. The dataset has been published as a resource at ACL 2020.</p>
<p>The dataset comes as a set of Gzip-compressed files containing line-based JSON in the Elasticsearch bulk format. Each data record consists of two lines:</p>
<p>{“index”: {“_id”: “&lt;urn:uuid:c1d95e4b-0f43-46c7-a99e-c575d1d8e1ce&gt;”}}
{“headers”: {“header name”: “header value”, …}, “text_plain”: “plaintext body”, “lang”: “en”, “segments”: [{“end”: 99, “label”: “paragraph”, “begin”: 0}, …], “group”: “gmane group name”}</p>
<p>The first line is the Elasticsearch index action with a document UUID, the second one the actual parsed email with a (reduced and anonymized) set of headers, the detected language, the original Gmane group name and the predicted content segments as character spans. The Gzip files are splittable every 1,000 records (line pairs) for parallel processing in, e.g., Hadoop.</p>
<p>Available email headers are:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>message_id
date (yyyy-MM-dd HH:mm:ssZZ)
subject
from
to
cc
in_reply_to
references
list_id
</pre></div>
</div>
<p>Available segment classes are:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>paragraph
closing
inline_headers
log_data
mua_signature
patch
personal_signature
quotation
quotation_marker
raw_code
salutation
section_heading
tabular
technical
visual_separator
</pre></div>
</div>
<p>Find more information about the dataset and the segmentation model at webis.de.</p>
<p>If you are using this resource in your work, please cite it as:</p>
<p>&#64;InProceedings{stein:2020o,
author =              {Janek Bevendorff and Khalid Al-Khatib and Martin Potthast and Benno Stein},
booktitle =           {58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)},
month =               jul,
publisher =           {Association for Computational Linguistics},
site =                {Seattle, USA},
title =               ,
year =                2020
}</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/3766985">https://zenodo.org/records/3766985</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3766985">https://doi.org/10.5281/zenodo.3766985</a></p>
</section>
<hr class="docutils" />
<section id="webis-scsmeta-2021">
<h2>Webis SCSmeta 2021<a class="headerlink" href="#webis-scsmeta-2021" title="Link to this heading">#</a></h2>
<p>Lars Meyer, Johannes Kiesel, Martin Potthast, Benno Stein</p>
<p>Published 2020-10-20</p>
<p>Licensed cc-by-4.0</p>
<p>This dataset is an extension of the Spoken Conversational Search dataset, specifically of the SCSdataset.csv, with annotations of types of meta-information mentioned in each turn.</p>
<p><a class="reference external" href="https://zenodo.org/records/4108196">https://zenodo.org/records/4108196</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4108196">https://doi.org/10.5281/zenodo.4108196</a></p>
</section>
<hr class="docutils" />
<section id="webis-systematic-reviews-for-computer-science-2025">
<h2>Webis Systematic Reviews for Computer Science 2025<a class="headerlink" href="#webis-systematic-reviews-for-computer-science-2025" title="Link to this heading">#</a></h2>
<p>Achkar, Pierre, Gollub, Tim, Potthast, Martin</p>
<p>Published 2025-09-20</p>
<p>Licensed cc-by-4.0</p>
<p>SR4CS: Systematic Reviews for Computer Science 
SR4CS is a benchmark dataset linking computer science systematic reviews (SRs) to their reported Boolean queries and curated reference pools, enabling reproducible research on Boolean query generation, retrieval effectiveness, and screening beyond the biomedical domain.</p>
<p>Total SRs: 1,212</p>
<p>Resolved references: 104,316 (incl. 89,447 with abstracts)</p>
<p>Tasks supported: Boolean query generation, retrieval benchmarking, screening</p>
<p> 
Paper: tba
Code: <a class="github reference external" href="https://github.com/webis-de/jcdl25-sr4cs">webis-de/jcdl25-sr4cs</a>
Contents
1) sr4cs.json
Primary dataset of SR entries.
A list of SR objects:
{
“id”: “1000688”,
“databases”: [“Google Scholar”, “Scopus”, “Web of Science”],
“search_strings_boolean”: [
“INTITLE (“green” OR “sustainab*”) AND INTITLE (“AI” OR “ML” OR “artificial intelligence” OR “machine learning” OR “deep learning”)”
],
“year_range”: “unbounded”,
“language_restrictions”: [“English”],
“inclusion_criteria”: [”…”],
“exclusion_criteria”: [”…”],
“topic”: “Green AI”,
“objective”: “Analyze Green AI literature …”,
“research_questions”: [“What are the characteristics …?”],
“snowballing”: true,
“sr_title”: “A systematic review of Green AI.”,
“sr_doi”: “10.1002/widm.1507”,
“sr_pdf_link”: “https://…”,
“ref_id”: [86, 113, 114, “…”],
“num_refs”: 97
}
Field summary</p>
<p>id (string): SR identifier (internal).</p>
<p>databases (list[str]): Databases reported in the SR.</p>
<p>search_strings_boolean (list[str]): Reported Boolean queries (verbatim).</p>
<p>year_range (string): Year bounds, e.g., “2018-2022” or “unbounded”.</p>
<p>language_restrictions (list[str]): Reported language constraints.</p>
<p>inclusion_criteria / exclusion_criteria (list[str]): As stated in the SR.</p>
<p>topic, objective (string): SR topic &amp; objective (verbatim).</p>
<p>research_questions (list[str]): RQs (verbatim).</p>
<p>snowballing (bool): Whether the SR reports snowballing/citation chasing.</p>
<p>sr_title, sr_doi, sr_pdf_link (metadata for the SR).</p>
<p>ref_id (list[int]): Integer IDs linking to the reference pool.</p>
<p>num_refs (int): Count of linked references for this SR.</p>
<ol class="arabic simple" start="2">
<li><p>sr4cs_with_sql.json
Same structure as sr4cs.json, plus translated SQL-style queries for uniform execution:</p></li>
</ol>
<p>sqlite_refined_queries (list[str]): SQLite FTS5 MATCH clauses translated from the reported Boolean strings (field-scoped to title/abstract).</p>
<ol class="arabic simple" start="3">
<li><p>Reference pool (metadata + abstracts)
The full reference pool is provided in three complementary formats:</p></li>
</ol>
<p>refs.db — SQLite database with FTS5 indexes (ready for querying)</p>
<p>refs.ndjson — Elasticsearch bulk dump (for index import)</p>
<p>refs.parquet — Tabular format (not indexed)</p>
<p>Reference fields (columns/keys):
ref_id, arxiv, author, citation-number, collection-title, container-title, date, director, doi, edition, editor, genre, id, isbn, issue, location, note, pages, pmcid, pmid, producer, publisher, raw, source, title, translator, type, url, volume, title_norm, abstract
Notes:</p>
<p>ref_id is the primary key linking SR entries to references.</p>
<p>title_norm is a normalized version of the title (lowercased, standardized spacing/characters) to support matching.</p>
<p>title_norm and abstract are prioritized fields for retrieval.</p>
<p>Many entries also include doi and url.</p>
<p>Abstract coverage is ~89k; the remainder provide title-only metadata.</p>
<p>Intended Use</p>
<p>Boolean query generation: Compare generated queries against transleted expert-reported queries and their retrieval on the shared reference pool.</p>
<p>Retrieval benchmarking: Evaluate title/abstract retrieval engines (e.g., SQLite FTS5, Elasticsearch) on realistic CS SR topics.</p>
<p>Screening experiments: Use the linked reference pools to prototype screening/prioritization pipelines.</p>
<p>Provenance &amp; Construction (summary)</p>
<p>Candidate SRs collected from DBLP by title search for “systematic review” on 2025-07-03, then filtered (peer-reviewed, OA, DOI, plausible page counts).</p>
<p>PDFs parsed to structured text; SR metadata (databases, queries, criteria, RQs, etc.) extracted and manually checked. </p>
<p>References extracted with a GROBID + AnyStyle pipeline; metadata resolved via Crossref, OpenAlex, Semantic Scholar, PubMed/Europe PMC; abstracts added where available.</p>
<p>File Inventory</p>
<p>sr4cs.json — SR entries with reported queries and linkage to reference IDs</p>
<p>sr4cs_with_sql.json — SR entries plus sqlite_refined_queries (rewritten Boolean queries for SQLite FTS5)</p>
<p>refs.db — SQLite database with reference metadata (FTS5 indexed)</p>
<p>refs.ndjson — Elasticsearch bulk dump of references (ready for import)</p>
<p>refs.parquet — Tabular reference metadata (not indexed)</p>
<p>Known Limitations</p>
<p>Reference coverage is large but not exhaustive; some entries are title-only.</p>
<p>Translated SQL queries approximate original database behavior; engine differences (tokenization, indexing, operators) can affect results.</p>
<p>FAIR &amp; Documentation</p>
<p>Findable: DOI, searchable metadata, stable identifiers (ref_id).</p>
<p>Accessible: Public files (JSON/SQLite/NDJSON).</p>
<p>Interoperable: Common schemas; JSON/SQL; ES bulk format.</p>
<p>Reusable: Development code and usage examples.</p>
<p>Contact</p>
<p>Pierre Achkar — <a class="reference external" href="mailto:pierre&#46;achkar&#37;&#52;&#48;uni-leipzig&#46;de">pierre<span>&#46;</span>achkar<span>&#64;</span>uni-leipzig<span>&#46;</span>de</a></p>
<p>Tim Gollub — <a class="reference external" href="mailto:tim&#46;gollub&#37;&#52;&#48;uni-weimar&#46;de">tim<span>&#46;</span>gollub<span>&#64;</span>uni-weimar<span>&#46;</span>de</a></p>
<p>Martin Potthast — <a class="reference external" href="mailto:martin&#46;potthast&#37;&#52;&#48;uni-kassel&#46;de">martin<span>&#46;</span>potthast<span>&#64;</span>uni-kassel<span>&#46;</span>de</a></p>
<p>If you build on SR4CS, we’d love to hear about your use case or results (Issues welcome in the GitHub repo).</p>
<p><a class="reference external" href="https://zenodo.org/records/17163932">https://zenodo.org/records/17163932</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.17163932">https://doi.org/10.5281/zenodo.17163932</a></p>
</section>
<hr class="docutils" />
<section id="webis-trigger-warning-corpus-2023">
<h2>Webis Trigger Warning Corpus 2023<a class="headerlink" href="#webis-trigger-warning-corpus-2023" title="Link to this heading">#</a></h2>
<p>Wiegmann, Wolska, Schröder, Borchardt, Stein, Potthast</p>
<p>Published 2023-05-27</p>
<p>Licensed cc-by-4.0</p>
<p>Abstract   A trigger warning or a content warning is intended to enable individuals to make an informed decision about whether to expose themselves to potentially distressing content. We introduce trigger warning assignment as a multilabel classification task and create the Webis Trigger Warning Corpus (WTWC), the first dataset of 1 million fanfiction works from the Archive of our Own with up to 36 different warnings per document. To provide a reliable catalog of trigger warnings, we carefully mapped institutionally-recommended trigger warnings against the millions of free-form tags assigned by fanfiction authors and organized them into the first comprehensive taxonomy of trigger warnings.</p>
<p> </p>
<p>Code for dehydration   <a class="github reference external" href="https://github.com/webis-de/ACL-23">webis-de/ACL-23</a></p>
<p> </p>
<p>Cite   </p>
<p>&#64;InProceedings{wiegmann:2023a,
address =               {Toronto, Canada},
author =                {Matti Wiegmann and Magdalena Wolska and Christopher Schr{&amp;quot;{o}}der and Ole Borchardt and Benno Stein and Martin Potthast},
booktitle =             {Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
month =                 jul,
publisher =             {Association for Computational Linguistics},
title =                 ,
year =                  2023
}</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/7976807">https://zenodo.org/records/7976807</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7976807">https://doi.org/10.5281/zenodo.7976807</a></p>
</section>
<hr class="docutils" />
<section id="webis-context-sensitive-word-search-queries-2022">
<h2>Webis-Context-sensitive-Word-Search-Queries-2022<a class="headerlink" href="#webis-context-sensitive-word-search-queries-2022" title="Link to this heading">#</a></h2>
<p>Matti Wiegmann, Michael Vöslke, Martin Potthast, Benno Stein</p>
<p>Published 2022-04-08</p>
<p>Licensed cc-by-4.0</p>
<p>This is the dataset created for Language Models as Context-sensitive Word Search Engines at the In2Writing workshop at ACL22.</p>
<p> </p>
<p>Cite </p>
<p>&#64;inproceedings{wiegmann:2022,
title =     “Language Models as Context-sensitive Word Search Engines”,
author =    “Wiegmann, Matti and V{“{o}}lske, Michael and Potthast, Martin and Stein, Benno”,
booktitle = “Proceedings of the 1st Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)”,
month =     may,
year =      “2022”,
address =   “Online”,
publisher = “Association for Computational Linguistics”,</p>
<p>Datasets</p>
<p>This repository contains two datasets with word search queries. Each word search query consists of a token n-gram with one wildcard token ([MASK]). The answers to each query are the most likely token to replace the mask. All queries originate from wikitext-103 and CLOTH, the respected source is annotated for each query.</p>
<p>The original-token dataset lists exactly one top answer for each query. The ranked-answers dataset lists multiple, sorted answers in three relevance categories, where 3 is the most relevant. Please refer to the citation for more details.</p>
<p><a class="reference external" href="https://zenodo.org/records/6425595">https://zenodo.org/records/6425595</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6425595">https://doi.org/10.5281/zenodo.6425595</a></p>
</section>
<hr class="docutils" />
<section id="webis-generated-game-art-23">
<h2>Webis-Generated-Game-Art-23<a class="headerlink" href="#webis-generated-game-art-23" title="Link to this heading">#</a></h2>
<p>Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast</p>
<p>Published 2023-01-10</p>
<p>Licensed cc-by-4.0</p>
<p>Generated images and case study report for the paper “The Infinite Index: Information Retrieval on Generative Text-To-Image Models”</p>
<p><a class="reference external" href="https://zenodo.org/records/7525482">https://zenodo.org/records/7525482</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7525482">https://doi.org/10.5281/zenodo.7525482</a></p>
</section>
<hr class="docutils" />
<section id="webis-stereo-21">
<h2>Webis-STEREO-21<a class="headerlink" href="#webis-stereo-21" title="Link to this heading">#</a></h2>
<p>Gienapp, Lukas, Kircheis, Wolfgang, Sievers, Bjarne, Stein, Benno, Potthast, Martin</p>
<p>Published 2021-10-18</p>
<p>Licensed cc-by-4.0</p>
<p>We present the Webis-STEREO-21 dataset, a massive collection of Scientific Text Reuse in Open-access publications. It contains more than 91 million cases of reused text passages found in 4.2 million unique open-access publications. Featuring a high coverage of scientific disciplines and varieties of reuse, as well as comprehensive metadata to contextualize each case, our dataset addresses the most salient shortcomings of previous ones on scientific writing. Webis-STEREO-21 allows for tackling a wide range of research questions from different scientific backgrounds, facilitating both qualitative and quantitative analysis of the phenomenon as well as a first-time grounding on the base rate of text reuse in scientific publications.</p>
<p>This is the open-access version of the dataset, which includes only the metadata of each reuse case. Due to licensing issues, the matched text is not included.</p>
<p><a class="reference external" href="https://zenodo.org/records/5575285">https://zenodo.org/records/5575285</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5575285">https://doi.org/10.5281/zenodo.5575285</a></p>
</section>
<hr class="docutils" />
<section id="webis-stereo-21-full-version">
<h2>Webis-STEREO-21 (Full Version)<a class="headerlink" href="#webis-stereo-21-full-version" title="Link to this heading">#</a></h2>
<p>Gienapp, Lukas, Kircheis, Wolfgang, Sievers, Bjarne, Stein, Benno, Potthast, Martin</p>
<p>Published 2021-12-21</p>
<p>We present the Webis-STEREO-21 dataset, a massive collection of Scientific Text Reuse in Open-access publications. It contains more than 91 million cases of reused text passages found in 4.2 million unique open-access publications. Featuring a high coverage of scientific disciplines and varieties of reuse, as well as comprehensive metadata to contextualize each case, our dataset addresses the most salient shortcomings of previous ones on scientific writing. Webis-STEREO-21 allows for tackling a wide range of research questions from different scientific backgrounds, facilitating both qualitative and quantitative analysis of the phenomenon as well as a first-time grounding on the base rate of text reuse in scientific publications.</p>
<p><a class="reference external" href="https://zenodo.org/records/5575320">https://zenodo.org/records/5575320</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5575320">https://doi.org/10.5281/zenodo.5575320</a></p>
</section>
<hr class="docutils" />
<section id="webis-web-archive-17">
<h2>Webis-Web-Archive-17<a class="headerlink" href="#webis-web-archive-17" title="Link to this heading">#</a></h2>
<p>Kiesel, Johannes, Potthast, Martin, Hagen, Matthias, Kneist, Florian, Stein, Benno</p>
<p>Published 2017-10-04</p>
<p>Licensed cc-by-sa-4.0</p>
<p>The Webis-Web-Archive-17 comprises a total of 10,000 web page archives from mid-2017 that were carefully sampled from the Common Crawl to involve a mixture of high-ranking and low-ranking web pages. The dataset contains the web archive files, HTML DOM, and screenshots of each web page, as well as per-page annotations of visual web archive quality. See this overview for all datasets that built upon this one. If you use this dataset in your research, please cite it using this paper.</p>
<p><a class="reference external" href="https://zenodo.org/records/4064019">https://zenodo.org/records/4064019</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4064019">https://doi.org/10.5281/zenodo.4064019</a></p>
</section>
<hr class="docutils" />
<section id="wie-generative-ki-arbeit-und-studium-verandert">
<h2>Wie Generative KI Arbeit und Studium verändert<a class="headerlink" href="#wie-generative-ki-arbeit-und-studium-verandert" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-06-20</p>
<p>Licensed cc-by-4.0</p>
<p>Dieses Slide-Deck führt Generative Künstliche Intelligenz zügig ein, umreißt einige Prompt-Engineeering Techniken und führt fortgeschrittene Techniken wie Wissensdestillation und Retrieval-Augmented-Generation ein. Verschiedene Auswirkungen auf das Lernen und Prüfen im Studium, sowie auf Felder wie Softwareentwicklung werden praktisch demonstriert.</p>
<p><a class="reference external" href="https://zenodo.org/records/15706181">https://zenodo.org/records/15706181</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15706181">https://doi.org/10.5281/zenodo.15706181</a></p>
</section>
<hr class="docutils" />
<section id="wie-generative-ki-unsere-tagliche-arbeit-verandert">
<h2>Wie Generative KI unsere tägliche Arbeit verändert<a class="headerlink" href="#wie-generative-ki-unsere-tagliche-arbeit-verandert" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-08-18</p>
<p>Licensed cc-by-4.0</p>
<p>In diesem Foliensatz erkunden wir verschiedene Szenarien in denen Generative Künstliche Intelligenz, speziell Sprachmodelle, unsere tägliche Arbeit verändert. Wir gehen ein auf Methoden der Informationsbeschaffung (Information Retrieval) und Prompt-Engineering. Dabei lernen wir wie Embeddings funktionieren und wie man mit ihrer Hilfe relevante Dokumente identifizieren kann, um mittels Retrieval Augmented Generation bessere Ergebnisse aus Sprachmodellen zu bekommen.</p>
<p><a class="reference external" href="https://zenodo.org/records/16894343">https://zenodo.org/records/16894343</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.16894343">https://doi.org/10.5281/zenodo.16894343</a></p>
</section>
<hr class="docutils" />
<section id="wie-generative-kunstliche-intelligenz-arbeit-und-studium-verandert">
<h2>Wie Generative Künstliche Intelligenz Arbeit und Studium verändert<a class="headerlink" href="#wie-generative-kunstliche-intelligenz-arbeit-und-studium-verandert" title="Link to this heading">#</a></h2>
<p>Haase, Robert</p>
<p>Published 2025-06-12</p>
<p>Licensed cc-by-4.0</p>
<p>Dieses Slide-Deck führt Generative Künstliche Intelligenz zügig ein, umreißt einige Prompt-Engineeering Techniken und führt fortgeschrittene Techniken wie Wissensdestillation und Retrieval-Augmented-Generation ein. Verschiedene Auswirkungen auf das Lernen und Prüfen im Studium, sowie auf Felder wie Softwareentwicklung werden praktisch demonstriert. Zum Schluß bekommen Teilnehmende einige Übungsaufgaben, die sie selbständig lösen können.</p>
<p><a class="reference external" href="https://zenodo.org/records/15646539">https://zenodo.org/records/15646539</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15646539">https://doi.org/10.5281/zenodo.15646539</a></p>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./domain"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../licenses/mit_license.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Mit license (27)</p>
      </div>
    </a>
    <a class="right-next"
       href="github.com.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Github.com (142)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Doi.org (52)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-4-se-exploring-hyperparameter-usage-and-tuning-in-machine-learning-research-cain-2023-submission">AI-4-SE/Exploring-Hyperparameter-Usage-And-Tuning-In-Machine-Learning-Research: CAIN-2023 Submission</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai4psychology-ki-kompetenz-training">AI4Psychology KI-Kompetenz-Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autonomous-ai-systems-for-data-analysis">Autonomous AI Systems for Data Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-science-lectures-2025-uni-leipzig-scads-ai">Bio-image Data Science Lectures 2025 @ Uni Leipzig / ScaDS.AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-literacy-open-science-and-collaborative-coding-and-ci-cd-slides-for-the-scads-ai-ga-workshop-working-with-data-and-code-like-a-pro">Data literacy, Open Science and Collaborative Coding and CI/CD - Slides for the ScaDS.AI GA-Workshop “Working with data and code like a pro”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#devops-hands-on-training">DevOps Hands-on Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#emnlp-23-bootstrapping-a-violence-detector-for-fan-fiction">EMNLP-23-Bootstrapping-a-Violence-Detector-for-Fan-Fiction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#european-union-artificial-intelligence-act-in-a-nutshell">European Union Artificial Intelligence Act (in a nutshell)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-machine-learning">Explainable Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-generative-ai-impacts-research-and-teaching">How Generative AI impacts research and teaching</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-llms-impact-bioimage-data-science">How LLMs impact BioImage Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-large-language-models-impact-bio-image-data-science">How Large Language Models impact Bio-Image Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ir-lab-leipzig-jena-summer-term-2023">IR Lab Leipzig Jena Summer Term 2023</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#this-loads-a-patched-version-of-ir-datasets-that-can-load-resources-from-tira">this loads a patched version of ir_datasets that can load resources from TIRA</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#this-patches-ir-datasets-and-loads-pyterrier-so-that-it-can-load-resources-from-tira-and-can-run-in-the-tira-sandbox">this patches ir_datasets and loads PyTerrier so that it can load resources from TIRA and can run in the TIRA sandbox</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ir-lab-leipzig-jena-winter-term-2023-2024">IR Lab Leipzig/Jena Winter Term 2023/2024</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">this loads a patched version of ir_datasets that can load resources from TIRA</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">this patches ir_datasets and loads PyTerrier so that it can load resources from TIRA and can run in the TIRA sandbox</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ki-kompetenz-training">KI-Kompetenz-Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-embeddings-of-stable-diffusion-prompts">Manipulating Embeddings of Stable Diffusion Prompts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pan23-trigger-detection">PAN23 Trigger Detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering-techniques-slides-for-the-scads-ai-ga-workshop-coding-effectively-with-ai-getting-started-with-cursor-and-copilot">Prompt Engineering Techniques - Slides for the ScaDS.AI GA-Workshop “Coding effectively with AI: Getting started with Cursor and Copilot”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducible-bio-image-analysis-using-python-napari-jupyter-and-ai">Reproducible Bio-Image Analysis using Python, Napari, Jupyter and AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-management">Research Data Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-sentiment-classification-train-dev-test-pair-ids">Same Sentiment Classification Train/Dev/Test Pair IDs</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#example-access-to-single-business-review-id">example access to single business/review id</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-side-stance-classification-adversarial-test-cases">Same Side Stance Classification Adversarial Test Cases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-side-stance-classification-resampled-datasets">Same Side Stance Classification Resampled Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-4-2025-ai-insights-on-ai-agents-agent-frameworks">ScaDS.AI Meetup #4 2025: AI Insights on AI Agents &amp; Agent-Frameworks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-4-2025-workshop-ai-agents">ScaDS.AI Meetup #4 2025: Workshop AI Agents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-8-2025-ai-insights-model-context-protocol-mcp">ScaDS.AI Meetup #8 2025: AI Insights Model Context Protocol (MCP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scads-ai-meetup-8-2025-workshop-model-context-protocol-mcp">ScaDS.AI Meetup #8 2025: Workshop Model Context Protocol (MCP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selected-meta-data-of-accepted-contributions-to-some-ai-ml-conferences-2020-2025">Selected meta data of accepted contributions to some AI/ML conferences 2020-2025</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#small-text-active-learning-for-text-classification-in-python">Small-Text: Active Learning for Text Classification in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#smart-slide-generation-re-using-training-materials-through-the-power-of-llms">Smart Slide Generation: Re-Using Training Materials Through the Power of LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#socio-technical-anti-patterns-in-building-ml-enabled-software-supplementary-material">Socio-Technical Anti-Patterns in Building ML-Enabled Software (Supplementary Material)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche-25-advertisement-in-retrieval-augmented-generation">Touché-25-Advertisement-in-Retrieval-Augmented-Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche20-argument-retrieval-for-controversial-questions">Touché20-Argument-Retrieval-for-Controversial-Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche21-argument-retrieval-for-controversial-questions">Touché21-Argument-Retrieval-for-Controversial-Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#touche22-image-retrieval-for-arguments">Touché22-Image-Retrieval-for-Arguments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#treesway">TreeSway</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-large-language-models-for-bio-image-analysis">Using Large Language Models for Bio-image Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-language-models-for-bio-image-data-science">Vision Language Models for Bio-image Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-abstractive-snippet-corpus-2020">Webis Abstractive Snippet Corpus 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-generated-native-ads-2024">Webis Generated Native Ads 2024</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-gmane-email-corpus-2019">Webis Gmane Email Corpus 2019</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-scsmeta-2021">Webis SCSmeta 2021</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-systematic-reviews-for-computer-science-2025">Webis Systematic Reviews for Computer Science 2025</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-trigger-warning-corpus-2023">Webis Trigger Warning Corpus 2023</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-context-sensitive-word-search-queries-2022">Webis-Context-sensitive-Word-Search-Queries-2022</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-generated-game-art-23">Webis-Generated-Game-Art-23</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-stereo-21">Webis-STEREO-21</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-stereo-21-full-version">Webis-STEREO-21 (Full Version)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webis-web-archive-17">Webis-Web-Archive-17</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wie-generative-ki-arbeit-und-studium-verandert">Wie Generative KI Arbeit und Studium verändert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wie-generative-ki-unsere-tagliche-arbeit-verandert">Wie Generative KI unsere tägliche Arbeit verändert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wie-generative-kunstliche-intelligenz-arbeit-und-studium-verandert">Wie Generative Künstliche Intelligenz Arbeit und Studium verändert</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Robert Haase
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on 2025-12-17.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Copyright: Licensed <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC-BY 4.0</a> unless mentioned otherwise. 
Contributions and feedback are welcome.
</p>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>